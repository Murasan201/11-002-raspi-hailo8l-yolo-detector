#!/usr/bin/env python3
"""
Raspberry Pi 5 + Hailo-8L YOLO物体検出アプリケーション
Hailo-8L AIアクセラレータを使用したリアルタイムYOLO物体検出システム
要件定義書: 11_002_raspi_hailo_8_l_yolo_detector.md

このスクリプトは以下の機能を提供します：
- リアルタイムカメラ映像からのYOLO物体検出
- Hailo-8L AIアクセラレータを使用した高速推論
- バウンディングボックス、クラス名、信頼度の表示
- 動画保存、ログ出力機能

使用方法：
    python raspi_hailo8l_yolo.py --res 1280x720 --conf 0.25 --iou 0.45
    python raspi_hailo8l_yolo.py --res 1280x720 --save
"""

import cv2
import numpy as np
import argparse
import time
import os
import csv
from datetime import datetime
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any

try:
    from picamera2 import Picamera2
    PICAMERA2_AVAILABLE = True
except ImportError:
    PICAMERA2_AVAILABLE = False
    print("警告: picamera2がインストールされていません。USB Webカメラモードで動作します。")

try:
    import hailo_platform
    from hailo_platform import (HEF, VDevice, HailoStreamInterface,
                               InferVStreams, ConfigureParams,
                               InputVStreamParams, OutputVStreamParams,
                               FormatType)
    HAILO_AVAILABLE = True
except ImportError:
    HAILO_AVAILABLE = False
    print("警告: HailoRT SDKがインストールされていません。CPUモードで動作します。")


class YOLODetector:
    """
    YOLO物体検出器クラス（Hailo-8L対応）
    Hailo-8L AIアクセラレータを使用したリアルタイム物体検出を実行します。
    初心者向けに設計され、前処理・推論・後処理が明確に分離されています。
    """

    def __init__(self, model_path: str, conf_threshold: float = 0.25,
                 iou_threshold: float = 0.45):
        """
        YOLODetectorの初期化

        Args:
            model_path (str): HEFモデルファイルのパス（例: 'models/yolov8s_h8l.hef'）
            conf_threshold (float): 信頼度閾値（0.0-1.0、デフォルト: 0.25）
            iou_threshold (float): IoU閾値（NMS用、デフォルト: 0.45）
        """
        self.model_path = model_path
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        self.device = None
        self.hef = None
        self.network_group = None
        self.input_vstreams_params = None
        self.output_vstreams_params = None
        self.input_name = None
        self.output_name = None

        # COCO データセットのクラス名
        self.class_names = [
            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck',
            'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench',
            'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra',
            'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',
            'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',
            'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',
            'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',
            'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',
            'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',
            'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',
            'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',
            'toothbrush'
        ]

        self._initialize_hailo()

    def _initialize_hailo(self) -> None:
        """
        Hailoデバイスとモデルの初期化
        HEFファイルを読み込んでHailo-8Lデバイスを設定します。
        新しいHailoRT API（v4.x）に対応しています。
        """
        if not HAILO_AVAILABLE:
            print("警告: HailoRT SDK が利用できません。CPUモードで動作します。")
            print("対処方法: Hailo SDK をインストールしてください")
            return

        if not os.path.exists(self.model_path):
            print(f"エラー: モデルファイルが見つかりません: {self.model_path}")
            print(f"対処方法: 以下のパスにHEFモデルを配置してください:")
            print(f"  {os.path.abspath('models/')}")
            print(f"例: python raspi_hailo8l_yolo.py --model models/yolov8n_hailo.hef")
            return

        try:
            # Hailo デバイスの初期化
            self.device = VDevice()

            # HEF（Hailo Execution Format）ファイルの読み込み
            self.hef = HEF(self.model_path)

            # 入出力ストリーム情報の取得
            input_vstream_infos = self.hef.get_input_vstream_infos()
            output_vstream_infos = self.hef.get_output_vstream_infos()

            # 入出力名の保存（推論時に使用）
            self.input_name = input_vstream_infos[0].name
            self.output_name = output_vstream_infos[0].name

            print(f"入力レイヤー: {self.input_name}")
            print(f"出力レイヤー: {self.output_name}")

            # ネットワークグループの設定（モデルをHailoデバイスにロード）
            configure_params = ConfigureParams.create_from_hef(
                hef=self.hef, interface=HailoStreamInterface.PCIe)
            network_groups = self.device.configure(self.hef, configure_params)
            self.network_group = network_groups[0]

            # 入出力VStreamsパラメータの作成
            # 入力: UINT8形式（0-255の画像データ）
            # 出力: FLOAT32形式（NMS後処理済みの検出結果）
            self.input_vstreams_params = InputVStreamParams.make(
                self.network_group, format_type=FormatType.UINT8)
            self.output_vstreams_params = OutputVStreamParams.make(
                self.network_group, format_type=FormatType.FLOAT32)

            print("Hailo-8L デバイスが正常に初期化されました。")

        except Exception as e:
            print(f"エラー: Hailo デバイスの初期化に失敗しました: {e}")
            print("対処方法: Hailo デバイスの接続と電源を確認してください")
            print("ヒント: hailortcli fw-control identify でデバイスを確認")
            self.device = None

    def preprocess_image(self, image: np.ndarray,
                        target_size: Tuple[int, int] = (640, 640)
                        ) -> np.ndarray:
        """
        画像の前処理（リサイズ、RGB変換）

        Args:
            image (np.ndarray): 入力画像（BGRフォーマット、OpenCVから取得）
            target_size (Tuple[int, int]): 目標サイズ (width, height)、デフォルト: (640, 640)

        Returns:
            np.ndarray: 前処理済み画像（UINT8形式、shape: (1, 640, 640, 3)）
        """
        # リサイズ：入力画像をモデルの入力サイズに統一
        resized = cv2.resize(image, target_size)

        # RGB変換：OpenCVはBGR形式ですが、YOLOはRGB形式を期待
        rgb_image = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)

        # HailoRT v4.x では入力は UINT8 形式（0-255）のまま使用
        # 正規化はHailo内部で自動的に行われる
        preprocessed = rgb_image.astype(np.uint8)

        # バッチ次元の追加：Hailo推論APIはバッチ処理に対応しているため、
        # （バッチサイズ, 高さ, 幅, チャンネル）形式に変更
        preprocessed = np.expand_dims(preprocessed, axis=0)

        return preprocessed

    def postprocess_detections(self, outputs: List,
                              original_shape: Tuple[int, int],
                              input_shape: Tuple[int, int] = (640, 640)
                              ) -> List[Dict[str, Any]]:
        """
        推論結果の後処理（座標変換）

        HailoRT v4.x のNMS後処理済み出力に対応しています。
        出力形式: [batch][class_id] = np.ndarray(N, 5)
        各検出は [y1, x1, y2, x2, confidence] の形式です。
        座標は0-1の正規化値で返されるため、ピクセル座標に変換します。

        Args:
            outputs (List): モデルの出力（NMS後処理済み）
            original_shape (Tuple[int, int]): 元画像のサイズ (height, width)
            input_shape (Tuple[int, int]): 入力画像のサイズ (height, width)

        Returns:
            List[Dict[str, Any]]: 検出結果のリスト。各辞書は以下を含む:
                - 'bbox' (list): バウンディングボックス座標 [x1, y1, x2, y2]
                - 'confidence' (float): 信頼度スコア（0.0-1.0）
                - 'class_id' (int): クラスID
                - 'class_name' (str): クラス名
        """
        detections = []

        if not outputs or len(outputs) == 0:
            return detections

        # HailoRT v4.x NMS後処理済み出力の解析
        # outputs[0] はバッチの最初の結果（1フレーム分）
        # outputs[0][class_id] は各クラスの検出結果 (N, 5) 形式
        batch_output = outputs[0]

        # 元画像のサイズ
        orig_h, orig_w = original_shape

        # 各クラスの検出結果を処理
        for class_id, class_detections in enumerate(batch_output):
            if not isinstance(class_detections, np.ndarray):
                continue

            if class_detections.size == 0:
                continue

            # 各検出を処理
            for detection in class_detections:
                # HailoRT NMS出力形式: [y1, x1, y2, x2, confidence]
                # 座標は0-1の正規化値
                y1_norm, x1_norm, y2_norm, x2_norm, confidence = detection

                if confidence < self.conf_threshold:
                    continue

                # 正規化座標を元画像のピクセル座標に変換
                # 正規化値（0-1）を直接元画像サイズに乗算
                x1 = int(x1_norm * orig_w)
                y1 = int(y1_norm * orig_h)
                x2 = int(x2_norm * orig_w)
                y2 = int(y2_norm * orig_h)

                # 座標の境界チェック
                x1 = max(0, min(x1, orig_w - 1))
                y1 = max(0, min(y1, orig_h - 1))
                x2 = max(0, min(x2, orig_w))
                y2 = max(0, min(y2, orig_h))

                # 有効なバウンディングボックスのみ追加
                if x2 > x1 and y2 > y1:
                    detections.append({
                        'bbox': [x1, y1, x2, y2],
                        'confidence': float(confidence),
                        'class_id': int(class_id),
                        'class_name': (self.class_names[class_id]
                                     if class_id < len(self.class_names)
                                     else f'class_{class_id}')
                    })

        # NMS は Hailo 側で既に適用済みなので、ここでは不要
        # ただし、信頼度順にソートして返す
        detections.sort(key=lambda x: x['confidence'], reverse=True)

        return detections

    def _apply_nms(self, detections: List[Dict[str, Any]]
                   ) -> List[Dict[str, Any]]:
        """
        Non-Maximum Suppression（NMS）の適用

        Args:
            detections (List[Dict[str, Any]]): 検出結果のリスト

        Returns:
            List[Dict[str, Any]]: NMS適用後の検出結果
        """
        if not detections:
            return detections

        # OpenCVのNMSを使用
        boxes = [det['bbox'] for det in detections]
        scores = [det['confidence'] for det in detections]

        indices = cv2.dnn.NMSBoxes(boxes, scores, self.conf_threshold, self.iou_threshold)

        if len(indices) > 0:
            indices = indices.flatten()
            return [detections[i] for i in indices]
        else:
            return []

    def detect(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """
        画像から物体検出を実行

        Args:
            image (np.ndarray): 入力画像（BGRフォーマット、任意の解像度）

        Returns:
            List[Dict[str, Any]]: 検出結果のリスト。各辞書は以下を含む:
                - 'bbox' (list): バウンディングボックス座標 [x1, y1, x2, y2]
                - 'confidence' (float): 信頼度スコア（0.0-1.0）
                - 'class_id' (int): クラスID（0-79、COCOデータセット）
                - 'class_name' (str): クラス名（例: 'person', 'car'）
        """
        if self.device is None or not HAILO_AVAILABLE:
            # Hailoが利用できない場合、CPUモードでのダミー検出（テスト用）
            return self._dummy_detect(image)

        try:
            # 前処理：画像をモデル入力形式に変換
            preprocessed = self.preprocess_image(image)

            # 推論実行：Hailo-8Lで高速推論を実行
            # HailoRT v4.x API を使用
            with self.network_group.activate():
                with InferVStreams(self.network_group,
                                  self.input_vstreams_params,
                                  self.output_vstreams_params) as infer_pipeline:
                    # 入力データの設定
                    input_dict = {self.input_name: preprocessed}

                    # 推論実行
                    output_dict = infer_pipeline.infer(input_dict)

                    # 出力の取得（NMS後処理済みの結果）
                    outputs = output_dict[self.output_name]

            # 後処理：推論結果から物体情報を抽出
            detections = self.postprocess_detections(outputs, image.shape[:2])

            return detections

        except Exception as e:
            print(f"エラー: 推論に失敗しました: {e}")
            print("対処方法: モデルファイルとHailoデバイスの接続を確認してください")
            import traceback
            traceback.print_exc()
            return []

    def _dummy_detect(self, image: np.ndarray) -> List[Dict[str, Any]]:
        """
        CPUモード用のダミー検出（Hailo非利用時のテスト用）

        Args:
            image (np.ndarray): 入力画像

        Returns:
            List[Dict[str, Any]]: テスト用の仮想検出結果
        """
        # Hailo非利用時のテスト用仮想検出結果
        # 実運用ではCPUベースのYOLO推論を実装することを推奨
        height, width = image.shape[:2]
        return [
            {
                'bbox': [width//4, height//4, width*3//4, height*3//4],
                'confidence': 0.85,
                'class_id': 0,
                'class_name': 'person'
            }
        ]


class CameraManager:
    """
    カメラ管理クラス
    Camera Module V3またはUSB Webカメラの初期化と制御を行います。
    自動フォールバック機能により、Picamera2が利用できない場合はUSBカメラに切り替わります。
    """

    def __init__(self, resolution: Tuple[int, int] = (1280, 720),
                 device_id: int = 0, flip_vertical: bool = False):
        """
        カメラマネージャーの初期化

        Args:
            resolution (Tuple[int, int]): カメラ解像度 (width, height)、デフォルト: (1280, 720)
            device_id (int): カメラデバイスID（USB Webカメラ用、デフォルト: 0）
            flip_vertical (bool): 画像を上下反転するかどうか（カメラを逆さまに設置した場合に使用）
        """
        self.resolution = resolution
        self.device_id = device_id
        self.flip_vertical = flip_vertical
        self.camera = None
        self.cap = None
        self.use_picamera = PICAMERA2_AVAILABLE

        self._initialize_camera()

    def _initialize_camera(self) -> None:
        """
        Picamera2（Camera Module V3）の初期化
        Picamera2が利用できない場合は、自動的にUSBカメラに切り替わります。
        """
        if self.use_picamera:
            try:
                self.camera = Picamera2()

                # Picamera2の設定（RGB888フォーマット）
                camera_config = self.camera.create_still_configuration(
                    main={"size": self.resolution, "format": "RGB888"}
                )
                self.camera.configure(camera_config)
                self.camera.start()

                print(f"Camera Module V3 を使用します。解像度: {self.resolution}")

            except Exception as e:
                print(f"警告: Camera Module V3 の初期化に失敗しました: {e}")
                print("対処方法: libcamera-hello でカメラ接続を確認してください")
                print("ヒント: raspi-config で camera インターフェースを有効化")
                self.use_picamera = False
                self._initialize_webcam()
        else:
            self._initialize_webcam()

    def _initialize_webcam(self) -> None:
        """
        USB Webカメラの初期化
        OpenCVのVideoCapture APIを使用してカメラを初期化します。
        """
        try:
            self.cap = cv2.VideoCapture(self.device_id)

            if not self.cap.isOpened():
                raise Exception(f"カメラデバイス {self.device_id} を開けません")

            # 解像度設定（リクエストベース。実際の解像度はカメラの対応状況に依存）
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.resolution[0])
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.resolution[1])

            print(f"USB Webカメラを使用します。解像度: {self.resolution}")

        except Exception as e:
            print(f"エラー: USB Webカメラの初期化に失敗しました: {e}")
            print("対処方法: カメラの接続を確認してください")
            print("ヒント: ls /dev/video* でカメラデバイスを確認")
            raise

    def read_frame(self) -> Optional[np.ndarray]:
        """
        カメラからフレームを読み取る

        Returns:
            Optional[np.ndarray]: フレーム画像（BGRフォーマット、shape: (height, width, 3)）
                読み取り失敗時はNoneを返す。flip_vertical=Trueの場合は上下反転済み。
        """
        try:
            frame = None

            if self.use_picamera and self.camera:
                # Picamera2からフレーム取得（RGB形式）
                frame = self.camera.capture_array()
                # RGB -> BGR 変換（OpenCVはBGR形式を使用）
                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)

            elif self.cap:
                # USB Webカメラからフレーム取得
                ret, frame = self.cap.read()
                if not ret:
                    return None

            # 上下反転処理（カメラの取り付け向きに対応）
            if frame is not None and self.flip_vertical:
                frame = cv2.flip(frame, 0)  # 0: 上下反転

            return frame

        except Exception as e:
            print(f"エラー: フレーム読み取りに失敗しました: {e}")
            return None

    def release(self) -> None:
        """
        カメラリソースの解放
        必ずアプリケーション終了時に呼び出してください。
        """
        if self.camera:
            self.camera.stop()
            self.camera.close()

        if self.cap:
            self.cap.release()


class DetectionLogger:
    """
    検出結果ログ管理クラス
    YOLOの検出結果をCSVフォーマットで記録します。
    タイムスタンプ、フレームID、クラス名、信頼度、バウンディングボックス座標を保存します。
    """

    def __init__(self, log_dir: str = "logs"):
        """
        ログ管理の初期化

        Args:
            log_dir (str): ログディレクトリ（デフォルト: 'logs'）
        """
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)

        # ログファイル名（タイムスタンプ付き）
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_file = self.log_dir / f"detections_{timestamp}.csv"

        # CSVヘッダーの書き込み
        with open(self.log_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['timestamp', 'frame_id', 'class_name', 'confidence', 'x1', 'y1', 'x2', 'y2'])

    def log_detections(self, frame_id: int,
                      detections: List[Dict[str, Any]]) -> None:
        """
        検出結果のCSVログ保存

        Args:
            frame_id (int): フレームID（0から始まる連番）
            detections (List[Dict[str, Any]]): 検出結果のリスト（YOLODetector.detect()の戻り値）
        """
        timestamp = datetime.now().isoformat()

        with open(self.log_file, 'a', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)

            for det in detections:
                bbox = det['bbox']
                writer.writerow([
                    timestamp, frame_id, det['class_name'],
                    det['confidence'], bbox[0], bbox[1], bbox[2], bbox[3]
                ])


def draw_detections(image: np.ndarray, detections: List[Dict[str, Any]]) -> np.ndarray:
    """
    画像に検出結果を描画します。
    バウンディングボックス、クラス名、信頼度スコアを画像上に描画します。

    Args:
        image (np.ndarray): 入力画像（BGRフォーマット）
        detections (List[Dict[str, Any]]): 検出結果のリスト（'bbox', 'class_name', 'confidence'を含む）

    Returns:
        np.ndarray: バウンディングボックスとラベル描画済み画像
    """
    result_image = image.copy()

    for det in detections:
        bbox = det['bbox']
        x1, y1, x2, y2 = bbox
        confidence = det['confidence']
        class_name = det['class_name']

        # バウンディングボックスの描画
        color = (0, 255, 0)  # 緑色
        cv2.rectangle(result_image, (x1, y1), (x2, y2), color, 2)

        # ラベルの描画
        label = f"{class_name}: {confidence:.2f}"
        label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]

        # ラベル背景
        cv2.rectangle(result_image,
                     (x1, y1 - label_size[1] - 10),
                     (x1 + label_size[0], y1),
                     color, -1)

        # ラベルテキスト
        cv2.putText(result_image, label,
                   (x1, y1 - 5),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

    return result_image


def draw_info(image: np.ndarray, fps: float, resolution: Tuple[int, int],
              inference_time: float) -> np.ndarray:
    """
    画像にパフォーマンス情報を描画します。
    FPS、解像度、推論時間を画面左上に表示します。

    Args:
        image (np.ndarray): 入力画像（BGRフォーマット）
        fps (float): 現在のフレームレート（フレーム/秒）
        resolution (Tuple[int, int]): カメラ解像度 (width, height)
        inference_time (float): 推論処理時間（ミリ秒）

    Returns:
        np.ndarray: パフォーマンス情報描画済み画像
    """
    result_image = image.copy()

    # 情報テキスト
    info_text = [
        f"FPS: {fps:.1f}",
        f"Resolution: {resolution[0]}x{resolution[1]}",
        f"Inference: {inference_time:.1f}ms"
    ]

    # 背景矩形
    text_height = 25
    bg_height = len(info_text) * text_height + 10
    cv2.rectangle(result_image, (10, 10), (300, bg_height), (0, 0, 0), -1)

    # テキスト描画
    for i, text in enumerate(info_text):
        y_pos = 30 + i * text_height
        cv2.putText(result_image, text, (15, y_pos),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

    return result_image


def parse_resolution(resolution_str: str) -> Tuple[int, int]:
    """
    解像度文字列をパースします。
    "1280x720" 形式の文字列から (width, height) タプルに変換します。

    Args:
        resolution_str (str): "1280x720" 形式の文字列

    Returns:
        Tuple[int, int]: (width, height) タプル

    Raises:
        argparse.ArgumentTypeError: フォーマットが無効な場合
    """
    try:
        width, height = map(int, resolution_str.split('x'))
        return (width, height)
    except ValueError:
        raise argparse.ArgumentTypeError(f"Invalid resolution format: {resolution_str}")


def main():
    """
    メイン関数：コマンドライン引数を処理してYOLO物体検出を実行
    Hailo-8Lを使用したリアルタイム推論とビデオ出力を管理します。
    """
    parser = argparse.ArgumentParser(
        description="Raspberry Pi 5 + Hailo-8L AI Kit YOLO物体検出",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    parser.add_argument('--model', type=str,
                       default='models/yolov8n_hailo.hef',
                       help='HEFモデルファイルのパス')

    parser.add_argument('--res', type=parse_resolution,
                       default=(1280, 720),
                       help='カメラ解像度 (例: 1280x720)')

    parser.add_argument('--conf', type=float, default=0.25,
                       help='信頼度閾値')

    parser.add_argument('--iou', type=float, default=0.45,
                       help='IoU閾値（NMS用）')

    parser.add_argument('--device', type=int, default=0,
                       help='カメラデバイスID')

    parser.add_argument('--save', action='store_true',
                       help='動画を保存する')

    parser.add_argument('--log', action='store_true',
                       help='検出結果をCSVログに保存する')

    parser.add_argument('--flip', action='store_true',
                       help='カメラ映像を上下反転する（カメラを逆さまに設置した場合）')

    args = parser.parse_args()

    print("=== Raspberry Pi Hailo-8L YOLO Detector ===")
    print(f"モデル: {args.model}")
    print(f"解像度: {args.res[0]}x{args.res[1]}")
    print(f"信頼度閾値: {args.conf}")
    print(f"IoU閾値: {args.iou}")
    if args.flip:
        print("カメラ映像: 上下反転")

    try:
        # YOLODetectorの初期化
        detector = YOLODetector(args.model, args.conf, args.iou)

        # カメラマネージャーの初期化
        camera = CameraManager(args.res, args.device, flip_vertical=args.flip)

        # ログ管理の初期化
        logger = None
        if args.log:
            logger = DetectionLogger()
            print(f"ログファイル: {logger.log_file}")

        # 動画保存の準備
        video_writer = None
        if args.save:
            output_dir = Path("output")
            output_dir.mkdir(exist_ok=True)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = output_dir / f"detection_{timestamp}.mp4"

            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            video_writer = cv2.VideoWriter(str(output_file), fourcc, 20.0, args.res)
            print(f"動画保存: {output_file}")

        # FPS計算用変数
        fps_counter = 0
        fps_time = time.time()
        current_fps = 0.0
        frame_id = 0

        print("\n物体検出を開始します。'q'キーで終了。")

        while True:
            # フレーム読み取り
            frame = camera.read_frame()
            if frame is None:
                print("フレームを取得できませんでした")
                break

            # 推論時間の測定
            inference_start = time.time()
            detections = detector.detect(frame)
            inference_time = (time.time() - inference_start) * 1000  # ms

            # 検出結果の描画
            result_frame = draw_detections(frame, detections)

            # 情報表示の描画
            result_frame = draw_info(result_frame, current_fps, args.res, inference_time)

            # ログ保存
            if logger:
                logger.log_detections(frame_id, detections)

            # 動画保存
            if video_writer:
                video_writer.write(result_frame)

            # 画面表示
            cv2.imshow('Hailo YOLO Detection', result_frame)

            # FPS計算
            fps_counter += 1
            if fps_counter % 10 == 0:
                current_time = time.time()
                current_fps = 10 / (current_time - fps_time)
                fps_time = current_time

            frame_id += 1

            # キー入力チェック（qキーまたはESCキーで終了）
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q') or key == ord('Q') or key == 27:  # 27 = ESC
                print("\n終了キーが押されました")
                break

    except KeyboardInterrupt:
        print("\n中断されました")

    except FileNotFoundError as e:
        print(f"エラー: ファイルが見つかりません: {e}")

    except Exception as e:
        print(f"エラー: 予期しないエラーが発生しました: {e}")
        import traceback
        traceback.print_exc()

    finally:
        # リソースの解放
        if 'camera' in locals():
            camera.release()

        if video_writer:
            video_writer.release()

        cv2.destroyAllWindows()
        print("アプリケーションを終了しました")


if __name__ == "__main__":
    main()